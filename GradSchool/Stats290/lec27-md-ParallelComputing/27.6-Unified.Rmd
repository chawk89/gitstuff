---
title: "27.6. Unified Paradigm for Sequential and Parallel Computing"
output: slidy_presentation
---

If your package can make use of parallel computing infrastructure,
then you have to account for the fact that the end user of your
package _may or may not_ have a parallel computing infrastructure.

Wouldn't it be nice if your package can take care of the parallel
infrastructure when it exists, and proceed sequentially when it does
not?

It is possible to design programs this way.

We saw how when we have looping constructs that can benefit from
parallelization, the package `foreach` in combination with `%dopar%`
lets you deal with both the sequential and parallel execution of code
in a unified way.


## 27.6.2. Some History

In the R world of parallel computing, it is important to know why we
have so many packages. It helps to know a bit of history.

- Initially, the `snow`, `multicore` packages were developed in
parallel (no pun intended!)

- This spawned `foreach`, `doSNOW`, `doMC` etc., as a way of iterating
  over cluster tasks.

- `doMC` is now subsumed by `doParallel` because `parallel` subsumed
`multicore`

- Now `parallel`, a recommended R package unified the entire approach
of `snow`, `multicore`.

- Naturally, we now have `doParallel` that matches
`parallel`. Approach is the same. See vignette in `doParallel`


## 27.6.3. KNN Example

To illustrate, let us consider a simple $K$-nearest neighbors
classification problem. As an illustration, I took the following code
from the
[internet](http://cbio.ensmp.fr/~jvert/svn/tutorials/practical/knnregression/knnregression.R)
and slightly modified it.


```{r}
# Practical session: kNN regression
## Jean-Philippe.Vert@mines.org
##
## In this practical session we:
##   - implement a simple version of regression with k nearest
##     neighbors (kNN)
##   - implement cross-validation for kNN
##   - measure the training, test and cross-validation error as a
##     function of k on the prostate cancer dataset
##

####################################
## Functions for k-nearest neighbors
####################################

knn <- function(klist, x.train, y.train, x.test) {
    ## k-nearest neighbors classification
    ##
    ## klist is a list of values of k to be tested
    ## x.train, y.train: the training set
    ## x.test: the test set
    ## Output: a matrix of predictions for the test set
    ##         (one column for each k in klist)
    ## Number of training and test examples
    n.train <- nrow(x.train)
    n.test <- nrow(x.test)

    ## Matrix to store predictions
    p.test <- matrix(NA, n.test, length(klist))

    ## Vector to store the distances of a point to the training points
    dsq <- numeric(n.train)

    ## Loop on the test instances
    for (tst in 1:n.test) {
        ## Compute distances to training instances
        for (trn in 1:n.train) {
            dsq[trn] <- sum((x.train[trn, ] - x.test[tst, ])^2)
        }

        ## Sort distances from smallest to largest
        ord <- order(dsq)

        ## Make prediction by averaging the k nearest neighbors
        for (ik in 1:length(klist)) {
            p.test[tst, ik] <- mean(y.train[ord[1:klist[ik]]])
        }
    }

    ## Return the matrix of predictions
    invisible(p.test)
}

knn.cv <- function(klist, x.train, y.train, nfolds) {
    ## Cross-validation for kNN
    ##
    ## Perform nfolds-cross validation of kNN, for the values of k in klist

    ## Number of instances
    n.train <- nrow(x.train)

    ## Matrix to store predictions
    p.cv <- matrix(NA, n.train, length(klist))

    ## Prepare the folds
    s <- split(sample(n.train),rep(1:nfolds,length=n.train))

    ## Cross-validation
    for (i in seq(nfolds)) {
        p.cv[s[[i]], ] <- knn(klist, x.train[-s[[i]], ],
                              y.train[-s[[i]]], x.train[s[[i]], ])
    }

    ## Return matrix of CV predictions
    invisible(p.cv)
}
```


## 27.6.4. `knnpack` Package

I took the above code and created a package out of it called
`knnpack`. __NOTE__ I did not add documentation, as that is not the
point of this exposition, so it will not pass `R CMD check`, but it
can be installed locally on your machine as usual.

```{r}
library(devtools)
if (!require("knnpack")) {
    devtools::install("./knnpack")
    library(knnpack)
}
```

___

## 27.6.5. Prediction Error Computation and Plotting

Once we have this package, we can write a function that returns the
various training, test and cross validated mean-squared error (MSE),
and also one that plots it.

```{r}
## Compute training, test and cross-validated MSE
computeMSE <- function(klist, x.train, y.train, x.test, nfolds) {
    y.pred.train <- knn(klist, x.train, y.train, x.train)
    y.pred.test <- knn(klist, x.train, y.train, x.test)
    y.pred.cv <- knn.cv(klist, x.train, y.train, nfolds)
    ## Compute mean-square error (MSE)
    mse.train <- apply((y.pred.train - y.train)^2, 2, mean)
    mse.test <- apply((y.pred.test - y.test)^2, 2, mean)
    mse.cv <- apply((y.pred.cv - y.train)^2, 2, mean)
    list(klist = klist,
         mse.train = mse.train,
         mse.test = mse.test,
         mse.cv = mse.cv)
}

library(ggplot2)
## Plot the MSE using ggplot2
plotMSE <- function(mseList) {
    ## Plot MSE as a function of k
    d <- data.frame(k = mseList$klist,
                    mse.train = mseList$mse.train,
                    mse.test = mseList$mse.test,
                    mse.cv = mseList$mse.cv)
    p <- ggplot(data = d, aes(x = k)) +
        geom_line(aes(y = mse.train, color = "mse.train")) +
        geom_line(aes(y = mse.test, color = "mse.test")) +
        geom_line(aes(y = mse.cv, color = "mse.cv")) + ylab("MSE")
    p
}
```


## 27.6.6. Prostate Data Example

We can now apply this to the prostate data from [Hastie, Tibshirani and
Friedman book](http://statweb.stanford.edu/~tibs/ElemStatLearn/).

```{r}
prost <- readRDS("prostate.RDS")
str(prost)
```

Create the training and test data sets

```{r}
x.train <- prost[prost$train, 1:8]
x.test <- prost[!prost$train, 1:8]
y.train <- prost[prost$train, 9]
y.test <- prost[!prost$train, 9]
```

___

We perform $k$-nearest regression prediction. Of course, we have to
choose $k$. We will explore values of $k$ from 1 to 20 and use 5-fold
cross-validation and plot the MSE.

```{r}
mseList <- computeMSE(klist = seq_len(20),
                      x.train = x.train,
                      y.train = y.train,
                      x.test = x.test,
                      nfolds = 5)
plotMSE(mseList)
```

So you'd choose a value of $k$ between 10 and 15 for example.


## 27.6.7. Parallel Cross validation

To take the same code and perform the cross-validation using the
unified approach, we will focus on the function `knn.cv` where the
cross validation will be done on various workers. Note the _minimal_
changes!

```{r, eval = FALSE}
new.knn.cv <- function(klist, x.train, y.train, nfolds) {
    ## Cross-validation for kNN
    ##
    ## Perform nfolds-cross validation of kNN, for the values of k in klist

    ## Number of instances
    n.train <- nrow(x.train)

    ## Matrix to store predictions
    p.cv <- matrix(NA, n.train, length(klist))

    ## Prepare the folds
    s <- split(sample(n.train),rep(1:nfolds, length = n.train))

    ## START OF CHANGES
    ## Cross-validation
    ## for (i in seq(nfolds)) {
    ##     p.cv[s[[i]], ] <- knn(klist, x.train[-s[[i]], ],
    ##                           y.train[-s[[i]]], x.train[s[[i]], ])
    ## }

    p <- foreach::foreach (i = seq(nfolds)) %dopar%  {
        knn(klist, x.train[-s[[i]], ], y.train[-s[[i]]], x.train[s[[i]], ])
    }

    for (i in seq(nfolds)) {
        p.cv[s[[i]], ] <- p[[i]]
    }
    ## END OF CHANGES

    ## Return matrix of CV predictions
    invisible(p.cv)
}
```

We change the `computeMSE` function to use the new function `new.knn.cv`.

```{r}
## Compute training, test and cross-validated MSE
new.computeMSE <- function(klist, x.train, y.train, x.test, nfolds) {
    y.pred.train <- knn(klist, x.train, y.train, x.train)
    y.pred.test <- knn(klist, x.train, y.train, x.test)
    y.pred.cv <- new.knn.cv(klist, x.train, y.train, nfolds) ## ONLY CHANGE!
    ## Compute mean-square error (MSE)
    mse.train <- apply((y.pred.train - y.train)^2, 2, mean)
    mse.test <- apply((y.pred.test - y.test)^2, 2, mean)
    mse.cv <- apply((y.pred.cv - y.train)^2, 2, mean)
    list(klist = klist,
         mse.train = mse.train,
         mse.test = mse.test,
         mse.cv = mse.cv)
}
```


## 27.6.8. Usage of Unified Code


First we do things the usual way, the sequential case, but using our
new functions.

___

### Sequential

```{r}
mseList <- new.computeMSE(klist = seq_len(20),
                          x.train = x.train,
                          y.train = y.train,
                          x.test = x.test,
                          nfolds = 5)
plotMSE(mseList)
```

___

### Parallel

Next, we do things the parallel way.

```{r}
library(doParallel)
cl <- makeCluster(5)
registerDoParallel(cl)
mseList <- new.computeMSE(klist = seq_len(20),
                          x.train = x.train,
                          y.train = y.train,
                          x.test = x.test,
                          nfolds = 5)
plotMSE(mseList)
stopCluster(cl)
```

___

The only difference between the two is that in the parallel case, the
user, that is me, registered a parallel backend. No other change to
actual code.

Thus, the _same_ code works utilizes the parallel infrastructure if it
exists, otherwise does the sequential execution. The user gets to
control whether to specify a parallel backend or not.


## 27.6.9. Resources

I would recommend:

- The vignette in the `parallel` package is worth reading.
- Parallel R, OReilly book, online for you.
- CRAN task view on High Performance Computing
- R Blogs (search for parallel)


## 27.6.10. Session Info

```{r}
sessionInfo()
```
